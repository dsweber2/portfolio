:PROPERTIES:
:header-args: :session *animal10 :kernel kaggle
:END:
#+title: Vision Convnet
The goal of this project is to do some basic image classification and to build an interpretable model. For this case, we'll use a relatively obscure and small [[https://www.kaggle.com/datasets/alessiocorrado99/animals10][Kaggle animal dataset]] with 10 animals: dog, cat, horse, spider, butterfly, chicken, sheep, cow, squirrel, and elephant.
Given that its of unknown quality, we'll first check to see if there's anything unusual about the images.
First, we use Kaggle's api to get the dataset via =kaggle datasets download -d alessiocorrado99/animals10=. The file structure is in Italian, so for my ease at least I'm going to translate the folder names to English using their brief dictionary
#+begin_src jupyter-python
import os
working_data_dir = "/fasterHome/workingDataDir/kaggle/animals10"
translate = {"cane": "dog", "cavallo": "horse", "elefante": "elephant", "farfalla": "butterfly", "gallina": "chicken", "gatto": "cat", "mucca": "cow", "pecora": "sheep", "scoiattolo": "squirrel", "ragno": "spider"}
for folder in os.listdir(os.path.join(working_data_dir, "raw-img")):
    contained = os.path.join(working_data_dir, "raw-img", folder)
    if folder in translate:
        os.rename(contained, os.path.join(working_data_dir, "raw-img", translate[folder]))
#+end_src

#+RESULTS:

And one more essential piece of housekeeping: import statements
#+begin_src jupyter-python
import torch
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.autograd as autograd
import torch.nn.functional as nnF
import torchvision as tv
import torchvision.transforms as transforms
import torchvision.transforms.functional as tvFun
from torch.utils.data import DataLoader
from torch import optim
from skimage import io, transform
#+end_src

#+RESULTS:
: /fasterHome/anaconda3/envs/kaggle/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
:   from .autonotebook import tqdm as notebook_tqdm

That out of the way, lets define some functions so that batches are collated to the same size.
#+begin_src jupyter-python
batch_size = 5

all_images = tv.datasets.ImageFolder(os.path.join(working_data_dir,"raw-img"), transform=tv.transforms.ToTensor())
def needed_pad(x,max_size):
    """pad `x` so that it's last two dimensions have size `max_size`"""
    diff = np.array(max_size) - np.array(x.shape[-2:])
    diff = diff/2
    return nnF.pad(x, (int(np.floor(diff[1])), int(np.ceil(diff[1])), int(np.floor(diff[0])), int(np.ceil(diff[0]))))

def collate_unevenly_sized(batch_iter):
    images, labels = zip(*batch_iter)
    max_size, _ = torch.max(torch.tensor([t.shape[-2:] for t in images]), dim=0)
    images = torch.cat([needed_pad(x, max_size).unsqueeze(0) for x in images], dim=0)
    return images, torch.tensor(labels)

loader = DataLoader(all_images, shuffle=True, collate_fn=collate_unevenly_sized, batch_size=20)
#+end_src

#+RESULTS:



Finally, lets take a look at some of the images:
#+begin_src jupyter-python
n_imgs = 25
ncols=5
plt.rcParams["savefig.bbox"] = 'tight'
fig, axs = plt.subplots(ncols=ncols, nrows=n_imgs//ncols, squeeze=False)
for ii, img in enumerate(loader):
    axs[ii//ncols, ii%ncols].imshow(torch.permute(img[0][0,:].squeeze(), (1,2,0)))
    axs[ii//ncols, ii%ncols].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])
    if ii==n_imgs-1:
        break
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/3ebd05b7f44853681f0743db6831163d3552a257.png]]

So we have a mix of pre-segmented images with a white background, more natural images with a background, and some cartoons. The variation in resolution is quite high, which could pose some difficulty; depending on how well or poorly the classifier does, we may want to upsample any particularly small images and perform more general ablations (translation, rotation, dilation etc). The subject is generally centered, generally takes up most of the image, and is frequently the only object present (except it appears horses).
* Defining a initial convnet
We define some convenience module classes first; the first gives us a resnet-like set of parallel convolution layers with filters of various sizes
#+begin_src jupyter-python
class MultiFilterLayer(nn.Module):
    """Maintains the size, applying 3 filterbanks of different sizes, then do a batch norm, and finally a mixing filter (1x1 convolution) that also subsamples. Generally inspired by the ResNet architecture."""

    def __init__(self, nchannels_in, nchannels_out, nonlin, norm_layer, filterSizes=(3,5,7), stride=2):
        super(MultiFilterLayer, self).__init__()
        self.norm = norm_layer(sum(nchannels_out[0:3]))
        self.nonlin = nonlin
        self.conv1 = nn.Conv2d(nchannels_in, nchannels_out[0], kernel_size=filterSizes[0], padding="same")
        self.conv2 = nn.Conv2d(nchannels_in, nchannels_out[1], kernel_size=filterSizes[1], padding="same")
        self.conv3 = nn.Conv2d(nchannels_in, nchannels_out[2], kernel_size=filterSizes[2], padding="same")
        self.conv_next = nn.Conv2d(sum(nchannels_out[0:3]), nchannels_out[3], kernel_size=1, stride=stride)

    def forward(self, x):
        x = torch.cat((self.conv1(x), self.conv2(x), self.conv3(x)), dim=1)
        x = self.norm(x)
        x = self.nonlin(x)
        x = self.conv_next(x)
        return x

class FullyConnected(nn.Module):
    """a fully connected set of layers, where `nodes_per_layer` is a list of the number of nodes in the ith layer for i>0, while the 0th entry is the size of the input. Between each layer is an application of the function `nonlin`."""
    def __init__(self, nodes_per_layer, nonlin = None):
        super(FullyConnected, self).__init__()
        if nonlin is None:
            nonlin = nn.ReLU6()
        self.layers = nn.ModuleList([nn.Linear(nodes_per_layer[ii-1], node) for (ii,node) in enumerate(nodes_per_layer) if ii>0])
        self.nonlin = nonlin

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
            x = self.nonlin(x)
        return nnF.log_softmax(x, dim=1)

class Animal10(nn.Module):
    """a set of multi-filter size convolutional layers, defined by `nchannels_multifilters` and `filterSizes`, followed by a set of fully connected layers, defined by `nfully_connected` (the first entry of n_fully_connected corresponds to the size of ). The nonlinearity `nonlin` is used univerally between all layers, while `norm_layer` defines the kind of batch norm used by the `MulitiFilterLayer`s."""
    def __init__(self, nchannels_multifilters, filterSizes, nfully_connected, nonlin = None, norm_layer = None, strides = None):
        super(Animal10, self).__init__()
        if nonlin is None:
            nonlin = nn.ReLU()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if strides is None:
            strides = [2 for _ in range(len(filterSizes))]

        # define the conv layers
        multiFilters = []
        for (ii, nchannels) in enumerate(nchannels_multifilters[:-1]):
            previous_out_channels = nchannels[-1]
            current_nchannels = nchannels_multifilters[ii+1]
            multiFilters.append(MultiFilterLayer(previous_out_channels, current_nchannels, nonlin, norm_layer, filterSizes[ii], strides[ii]))
        self.multiFilters = nn.ModuleList(multiFilters)
        self.adaptiveAve =  nn.AdaptiveAvgPool2d((1,1))
        # define the fully connected layers
        self.fullyConnected = FullyConnected([nchannels_multifilters[-1][-1], *nfully_connected], nonlin)

    def forward(self, x):
        for layer in self.multiFilters:
            x = layer(x)
        x = self.adaptiveAve(x)
        x = torch.flatten(x, 1) # drop the spatial components
        x = self.fullyConnected(x)
        return x
#+end_src

#+RESULTS:


Given that general framework, lets make a network with 3 convolutional layers
#+begin_src jupyter-python
n_channels = ([3], [16,16,16,48], [16,16,16,48], [32,32,32,64])
filterSizes = ([3,5,7], [3,5,5], [3,3,3])
fully_connected = (256,256,256, 10)
strides = [2,4,4]
animal_classifier = Animal10(n_channels, filterSizes, fully_connected, strides = strides)
#+end_src

#+RESULTS:
* GPU doesn't play nicely with notebooks
#+begin_src jupyter-python
del animal_classifier
torch.cuda.empty_cache()
gc.collect()
#+end_src

#+RESULTS:
: 5197

To the GPU
#+begin_src jupyter-python
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
import gc
gc.collect()
torch.cuda.empty_cache()
device = torch.device("cuda")
animal_classifier = animal_classifier.to(device=device)
for example_batch in loader:
    break
#animal_classifier(example_batch[0])
animal_classifier(example_batch[0].to(device))
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
[0;31m---------------------------------------------------------------------------[0m
[0;31mRuntimeError[0m                              Traceback (most recent call last)
Cell [0;32mIn[42], line 11[0m
[1;32m      9[0m     [38;5;28;01mbreak[39;00m
[1;32m     10[0m [38;5;66;03m#animal_classifier(example_batch[0])[39;00m
[0;32m---> 11[0m [43manimal_classifier[49m[43m([49m[43mexample_batch[49m[43m[[49m[38;5;241;43m0[39;49m[43m][49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m[43mdevice[49m[43m)[49m[43m)[49m

File [0;32m/fasterHome/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py:1130[0m, in [0;36mModule._call_impl[0;34m(self, *input, **kwargs)[0m
[1;32m   1126[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1127[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1128[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1129[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1130[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[38;5;28;43minput[39;49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1131[0m [38;5;66;03m# Do not call functions when jit is used[39;00m
[1;32m   1132[0m full_backward_hooks, non_full_backward_hooks [38;5;241m=[39m [], []

Cell [0;32mIn[4], line 59[0m, in [0;36mAnimal10.forward[0;34m(self, x)[0m
[1;32m     57[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;28mself[39m, x):
[1;32m     58[0m     [38;5;28;01mfor[39;00m layer [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mmultiFilters:
[0;32m---> 59[0m         x [38;5;241m=[39m [43mlayer[49m[43m([49m[43mx[49m[43m)[49m
[1;32m     60[0m     x [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39madaptiveAve(x)
[1;32m     61[0m     x [38;5;241m=[39m torch[38;5;241m.[39mflatten(x, [38;5;241m1[39m) [38;5;66;03m# drop the spatial components[39;00m

File [0;32m/fasterHome/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py:1130[0m, in [0;36mModule._call_impl[0;34m(self, *input, **kwargs)[0m
[1;32m   1126[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1127[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1128[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1129[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1130[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[38;5;28;43minput[39;49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1131[0m [38;5;66;03m# Do not call functions when jit is used[39;00m
[1;32m   1132[0m full_backward_hooks, non_full_backward_hooks [38;5;241m=[39m [], []

Cell [0;32mIn[4], line 15[0m, in [0;36mMultiFilterLayer.forward[0;34m(self, x)[0m
[1;32m     13[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;28mself[39m, x):
[1;32m     14[0m     x [38;5;241m=[39m torch[38;5;241m.[39mcat(([38;5;28mself[39m[38;5;241m.[39mconv1(x), [38;5;28mself[39m[38;5;241m.[39mconv2(x), [38;5;28mself[39m[38;5;241m.[39mconv3(x)), dim[38;5;241m=[39m[38;5;241m1[39m)
[0;32m---> 15[0m     x [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mnorm[49m[43m([49m[43mx[49m[43m)[49m
[1;32m     16[0m     x [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mnonlin(x)
[1;32m     17[0m     x [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mconv_next(x)

File [0;32m/fasterHome/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py:1130[0m, in [0;36mModule._call_impl[0;34m(self, *input, **kwargs)[0m
[1;32m   1126[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1127[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1128[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1129[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1130[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[38;5;28;43minput[39;49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1131[0m [38;5;66;03m# Do not call functions when jit is used[39;00m
[1;32m   1132[0m full_backward_hooks, non_full_backward_hooks [38;5;241m=[39m [], []

File [0;32m/fasterHome/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:168[0m, in [0;36m_BatchNorm.forward[0;34m(self, input)[0m
[1;32m    161[0m     bn_training [38;5;241m=[39m ([38;5;28mself[39m[38;5;241m.[39mrunning_mean [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m) [38;5;129;01mand[39;00m ([38;5;28mself[39m[38;5;241m.[39mrunning_var [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m)
[1;32m    163[0m [38;5;124mr[39m[38;5;124;03m"""[39;00m
[1;32m    164[0m [38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be[39;00m
[1;32m    165[0m [38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are[39;00m
[1;32m    166[0m [38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).[39;00m
[1;32m    167[0m [38;5;124;03m"""[39;00m
[0;32m--> 168[0m [38;5;28;01mreturn[39;00m [43mF[49m[38;5;241;43m.[39;49m[43mbatch_norm[49m[43m([49m
[1;32m    169[0m [43m    [49m[38;5;28;43minput[39;49m[43m,[49m
[1;32m    170[0m [43m    [49m[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated[39;49;00m
[1;32m    171[0m [43m    [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mrunning_mean[49m
[1;32m    172[0m [43m    [49m[38;5;28;43;01mif[39;49;00m[43m [49m[38;5;129;43;01mnot[39;49;00m[43m [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mtraining[49m[43m [49m[38;5;129;43;01mor[39;49;00m[43m [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mtrack_running_stats[49m
[1;32m    173[0m [43m    [49m[38;5;28;43;01melse[39;49;00m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m,[49m
[1;32m    174[0m [43m    [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mrunning_var[49m[43m [49m[38;5;28;43;01mif[39;49;00m[43m [49m[38;5;129;43;01mnot[39;49;00m[43m [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mtraining[49m[43m [49m[38;5;129;43;01mor[39;49;00m[43m [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mtrack_running_stats[49m[43m [49m[38;5;28;43;01melse[39;49;00m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m,[49m
[1;32m    175[0m [43m    [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mweight[49m[43m,[49m
[1;32m    176[0m [43m    [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mbias[49m[43m,[49m
[1;32m    177[0m [43m    [49m[43mbn_training[49m[43m,[49m
[1;32m    178[0m [43m    [49m[43mexponential_average_factor[49m[43m,[49m
[1;32m    179[0m [43m    [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43meps[49m[43m,[49m
[1;32m    180[0m [43m[49m[43m)[49m

File [0;32m/fasterHome/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/functional.py:2438[0m, in [0;36mbatch_norm[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)[0m
[1;32m   2435[0m [38;5;28;01mif[39;00m training:
[1;32m   2436[0m     _verify_batch_size([38;5;28minput[39m[38;5;241m.[39msize())
[0;32m-> 2438[0m [38;5;28;01mreturn[39;00m [43mtorch[49m[38;5;241;43m.[39;49m[43mbatch_norm[49m[43m([49m
[1;32m   2439[0m [43m    [49m[38;5;28;43minput[39;49m[43m,[49m[43m [49m[43mweight[49m[43m,[49m[43m [49m[43mbias[49m[43m,[49m[43m [49m[43mrunning_mean[49m[43m,[49m[43m [49m[43mrunning_var[49m[43m,[49m[43m [49m[43mtraining[49m[43m,[49m[43m [49m[43mmomentum[49m[43m,[49m[43m [49m[43meps[49m[43m,[49m[43m [49m[43mtorch[49m[38;5;241;43m.[39;49m[43mbackends[49m[38;5;241;43m.[39;49m[43mcudnn[49m[38;5;241;43m.[39;49m[43menabled[49m
[1;32m   2440[0m [43m[49m[43m)[49m

[0;31mRuntimeError[0m: CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 7.92 GiB total capacity; 4.67 GiB already allocated; 202.81 MiB free; 5.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
#+end_example
:END:

* Training Said Convnet

Initially, lets default to stochastic gradient descent (with momentum), and a [[https://en.wikipedia.org/wiki/Cross_entropy][cross-entropy loss]].
#+begin_src jupyter-python
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(animal_classifier.parameters(), lr=0.001, momentum=0.9)
#+end_src

#+RESULTS:


#+begin_src jupyter-python :async yes
loss_record = []
for epoch in range(1):
    running_loss = 0.0
    for ii, data in enumerate(loader):
        image_batch, labels = data

        optimizer.zero_grad()

        class_pred = animal_classifier(image_batch)
        loss = criterion(class_pred, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if ii % 2000 == 1999:
            print(f"[{epoch}, {ii + 1:5d}] loss: {running_loss / 2000:.3f}")
            loss_record.append(running_loss)
            running_loss = 0
        if ii % 8000 == 0:
            break
#+end_src

#+RESULTS:

Since there are 10 classes, random guessing gets us a Cross entropy of $-\log(.1) \approx 2.302$, which is around the loss that we start with.
* Doing some transfer learning
* Building an interpretable model
